### From this post: http://stats.stackexchange.com/q/232959/67822

# Noiseless training data:
Xtrain = c(-4,-3, -2, -1, 1)
ytrain = sin(Xtrain)

# Apply the kernel function to our training points:
K_train = kernel(Xtrain, Xtrain, param)

Ch_train = chol(K_train + 0.00005 * diag(length(Xtrain)))

# Compute the mean at our test points:

K_trte = kernel(Xtrain, Xtest, param)
core = solve(Ch_train) %*% K_trte
temp = solve(Ch_train) %*% ytrain
mu = t(core) %*% temp

# Compute the standard deviation:

tempor = colSums(core^2)

# Notice that all.equal(diag(t(Lk) %*% Lk), colSums(Lk^2)) TRUE

s2 = diag(K_test) - tempor
stdv = sqrt(s2)

# Draw samples from the posterior at our test points:

Ch_post_gener = chol(K_test + 1e-6 * diag(n) - (t(core) %*% core))
m_prime = matrix(rnorm(n * 3), ncol = 3)
sam = Ch_post_gener %*% m_prime
f_post = as.vector(mu) + sam



colors=c(2, "darkred","blue")
plot(Xtest,f_post[,1], type="l", lwd = 2, col='darkorange', 
     ylim=c(-2.5,2.5),
     xlab='',ylab='') 
title(main="Three samples from the GP Posterior
      Training points added",
      cex.main=0.85)

abline(h = 0)

for(i in 2:3){
  lines(Xtest, f_post[,i], type = 'l', lwd=2, col=colors[i]) # Plotting
}
points(Xtrain, ytrain, pch = 20, cex=2)

