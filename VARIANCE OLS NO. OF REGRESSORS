# Simulation for this post: http://stats.stackexchange.com/a/195018/67822
# The data is from mtcars, but just a few variables:
attach(mtcars)
head(mtcars)
dat <- cbind(mpg, wt, hp) # First data set
fit <- lm(mpg ~ wt + hp)  # First OLS
summary(fit)

# MANUAL CALCULATION OF VARIANCE FOR wt:
# Model matrix X
model.matrix(fit)

#Variance-covariance matrix:
Var_Cov = t(model.matrix(fit)) %*% model.matrix(fit)
Inv_Var_Cov = solve(Var_Cov) # This is the (XtX)' that we will need to multiply times MSE

#Variance estimate:
# SSE / (n - p) # p being the number of parameters
# residuals(fit) # same as: mpg - fitted(fit)

(MSE = sum(residuals(fit)^2)/(32 - 3)) # Counting the intercept as a parameter
(MSE = sum((mpg - fitted(fit))^2)/(32 - 3)) # This gives us the same result

# MSE is the same as RSS (or SSE) divided by error or residual degrees of freedom 
# no. observations − no. indepen't variables − 1
# http://stats.stackexchange.com/a/172161/67822

#Putting it together, the variance (well, standard error) for weight is:

sqrt(MSE * Inv_Var_Cov) # The standard errors for each parameter in diag

diag(sqrt(MSE * Inv_Var_Cov)) # It is found in the diagonal

summary(fit)$coefficients[4:6] # This is what R gives us in the ouput

# INCREASING THE NUMBER OF REGRESSORS:

dat2 <- cbind(dat, disp) # Second data set
fit2 <- lm(mpg ~ wt + hp + disp) # Second OLS
summary(fit2)
model.matrix(fit2)
Var_Cov2 = t(model.matrix(fit2)) %*% model.matrix(fit2)
Inv_Var_Cov2 = solve(Var_Cov2)

(MSE2 = sum(residuals(fit2)^2)/(32 - 4))
sqrt(MSE2 * Inv_Var_Cov2)
summary(fit2)$coefficients

# So the variance for weight is higher, but why?

sum(residuals(fit2)^2) / sum(residuals(fit)^2) # Better fit with model 2
MSE2/MSE # [1] 1.035411                        # Less degrees of freedom bring up the MSE for model 2.

What does the entry look like in the weight-weight intersect of the inverse of AtA?

Inv_Var_Cov2[2,2]/Inv_Var_Cov[2,2]     # 170 % increase with model 2!!!
# 0.1632352 / 0.0595249

# Why? There was no change in Var_Cov2[2,2]/Var_Cov[2,2]

Var_Cov2[2,2]/Var_Cov[2,2]

# Is it the determinant of Var_Cov?

library(Matrix)
det(Var_Cov2)/det(Var_Cov) # [1] 65012.45 # It's certainly higher in model 2, and since det is in the denom...

# Testing that the determinants are correctly calculated:
# prod(diag(expand(lu(Var_Cov))$U))
#[1] 78341330
# det(Var_Cov)
#[1] 78341330
# prod(diag(expand(lu(Var_Cov2))$U))
#[1] 5.093162e+12
# det(Var_Cov2)
# 5.093162e+12

# The determinant goes in the denom. and it's higher in second
# model. Hence it would reduce the variance in the end.

#What is the matrix of cofactors for weight:
det(Var_Cov[c(1,3),c(1,3)])
det(Var_Cov2[c(1,3,4),c(1,3,4)])

#Does it overcome the influence of the determinant?

(1 / det(Var_Cov)) * det(Var_Cov[c(1,3),c(1,3)])
(1/ det(Var_Cov2)) * det(Var_Cov2[c(1,3,4),c(1,3,4)])

# Aha! It's the matrix of cofactors!!!!

# So what about the confidence interval:

summary(fit)$coef
beta_hat <- summary(fit)$coef[2,1]
Std.Err <- summary(fit)$coef[2,2]
beta_hat + c(-1,1) * Std.Err * qt(0.975, 30)
confint(fit, 'wt', level=0.95)

# And the t-value:

beta_hat/Std.Err # So the higher the Std.Err, the lower the Pr sig.


# Alternative variance formula:

alt <- lm(wt ~ hp)
R_square = summary(alt)$r.squared
var_wt = var(wt)

sqrt(MSE)/sqrt(31 * var_wt * (1 - R_square)) # 31 cause intercept doesn't count.
summary(fit)

# So the better the other co-regressors explain wt, the higher
# the r.squared, and the higher the variance of its parameter estimate.
