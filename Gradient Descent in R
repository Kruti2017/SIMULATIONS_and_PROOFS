## http://www.r-bloggers.com/linear-regression-by-gradient-descent/

library(R.matlab)
# If the dataset is not already there, download from GoogleDrive on this link:
# https://drive.google.com/open?id=0Bwl-HpVJ_5PeeXNWYnhDZzBGTmM
# and place the file in the R directory.
data <- readMat('data.mat')
str(data)

X = data$X
y = data$y
y = replace(y, y==10, 0)


# In X we have 5000 handwritten numbers, each one digitized in 400 elements
# corresponding to any of the rows of X
# y is a 5000 x 1 matrix with the "answers".

# An example would be:
set.seed(5)
Z = X[sample(nrow(X), size = 1 ,replace=F),]
Z = matrix(Z, nrow = 20, byrow= T)

par(mar = rep(0, 4))
image(Z, axes = FALSE, col = grey(seq(0, 1, length = 256)))


######################################
# COST FUNCTION LOGISTIC REGRESSION: #
######################################

#One versus all example trying to classify which is number 9 and which isn't:

y = replace(y, y!=9, 0)
y = replace(y, y==9, 1)

# First the activation sigmoid function:

sigmoid = function(X, theta){
  1 / (1 + exp(-(X %*% theta)))
}

# Introducing a bias term or intercept:
X = cbind(rep(1, nrow(X)), X) # We will have a theta for the intercept


# The regression cost function (residuals, sort of...):

cost = function(X, y, theta, lambda=0.1){

  hypothesis = sigmoid(X, theta)
  
# The regularized cost function is:
m = length(y)
J = (1/m) * sum(-y * log(hypothesis)  - (1 - y) * log(1 - hypothesis)) +
(lambda/(2 * m)) * sum(theta[2:nrow(theta), 1]^2)
J
}


# gradient descent:

m=nrow(X)
  
# initialize coefficients
theta = matrix(rep(0, ncol(X)), ncol=1)

# learning rate and iteration limit
alpha <- 0.01
num_iters <- 100

# regularization parameter:
lambda=0.1

# keep history
cost_history  = matrix(rep(0, num_iters), ncol=1)
theta_history = matrix(rep(0, ncol(X) * num_iters), ncol= num_iters)
grad          = matrix(rep(0, ncol(X)), ncol= 1)

for (i in 1:num_iters) {
  cost_history[i] = cost(X, y, theta)
  grad[1,] = (1/m) * (t(X[,1]) %*% (sigmoid(X, theta) - y))
  grad[2:nrow(theta),] = 1/m * t(X[,2:ncol(X)]) %*% (sigmoid(X, theta) - y) +
    lambda * theta[2:nrow(theta),]
 
  theta = theta - alpha * grad
  
  theta_history[,i] <- theta
}

dev.off()
plot(cost_history, type="l", lwd=2, col=4, main='Cost function', ylab='cost', xlab='Iterations')

